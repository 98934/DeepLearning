"""
@Author Wu Wei
正则化逻辑回归，一对多分类。
"""

import numpy as np
from sklearn.preprocessing import PolynomialFeatures
import scipy.optimize as opt
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt


class LogisticRegression:
    """逻辑回归算法，监督学习，对数据进行0-1分类，有正则化。

    Parameters
    ----------
        poly_degree : int, default 1.
            The degree of polynomial.

        poly : sklearn.preprocessing.PolynomialFeatures.
            Generated by poly_degree with the class PolynomialFeatures.

        result : scipy.optimize.minimize.
            The result of the self.fit() method, including the final theta and etc.

        learning_rate : float, default 1.0.
            The parameter of regularization term.
    """

    poly_degree = 1
    poly = None
    result = None
    learning_rate = 1.0

    def __init__(self, poly_degree=1, learning_rate=1.0):
        self.poly_degree = poly_degree
        self.poly = PolynomialFeatures(self.poly_degree)
        self.learning_rate = learning_rate

    @staticmethod
    def sigmoid(z):
        return 1 / (1 + np.exp(-z))

    @staticmethod
    def feature_scaling(X):
        """
        Get every feature into approximately a -1 <= x_i <= 1 range.
        It could fasten the self.fit() method.
        x_i = (x_i - x.mean()) / x.std().

        :param X: numpy.ndarray(m, n).
        :return: X: numpy.ndarray(m, n), the data with standardization.
        """
        return (X - X.mean(axis=0)) / X.std(axis=0)

    def prepare_data(self, X, standardize=False):
        """
        数据预处理

        :param X: numpy.ndarray(m, n), raw data.
        :param standardize: boolean, default False.
            If Ture, X will be standardized along axis(default axis=0).
        :return: numpy.ndarray(m, n), increased some features
        """

        if standardize:
            X = self.feature_scaling(X)

        X = self.poly.fit_transform(X)
        return X

    def hypothesis(self, theta, X):
        """
        假设函数

        :param theta: numpy.ndarray(n,)
        :param X: numpy.ndarray(m, n)
        :return: numpy.ndarray(m, 1), sigmoid(X @ theta.T)
        """

        theta = theta.reshape(X.shape[1], 1)
        return self.sigmoid(X @ theta)

    def cost(self, theta, X, y):
        """
        正则化的代价函数，采用log形式可变成凸函数

        :param theta: numpy.ndarray(n,)
        :param X: numpy.ndarray(m, n)
        :param y: numpy.ndarray(m, 1)
        :return: numpy.float64, the cost without regularity
        """

        first = np.multiply(-y, np.log(self.hypothesis(theta, X)))
        second = np.multiply((1 - y), np.log(1 - self.hypothesis(theta, X)))

        regularized_term = (self.learning_rate / (2 * X.shape[0])) * np.power(theta[1:], 2).sum()

        return np.mean(first - second) + regularized_term

    def gradient(self, theta, X, y):
        """
        正则化的代价函数的梯度。

        :param theta: numpy.ndarray(n,).
        :param X: numpy.ndarray(m, n).
        :param y: numpy.ndarray(m, 1).
        :return: numpy.ndarray(n,), gradient values of every theta.
        """

        inner = X.T @ (self.hypothesis(theta, X) - y)
        grad = inner / X.shape[0]

        regularized_term = (self.learning_rate / X.shape[0]) * theta[1:]
        regularized_term = np.concatenate([np.array([0]), regularized_term])

        return grad.ravel() + regularized_term

    def fit(self, X, y):
        """
        拟合函数

        :param X: numpy.ndarray(m, n), training data, m number, n features
        :param y: numpy.ndarray(m, 1), training label, m number, the value is 1 or 0
        :return: self, LogisticRegression
        """
        X = self.prepare_data(X)
        m, n = X.shape
        theta = np.zeros(n)

        self.result = opt.minimize(fun=self.cost,
                                   x0=theta,
                                   args=(X, y),
                                   method='TNC',
                                   jac=self.gradient)

        return self

    def predict(self, X):
        """
        预测数据

        :param X: numpy.ndarray(m, n), input data
        :return: numpy.ndarray(m, 1), prediction value
        """
        X = self.prepare_data(X)
        theta = self.result.x
        prob = self.hypothesis(theta, X)

        return (prob >= 0.5).astype(int)

    def report(self, X, y):
        """
        分类报告，可查看准确率等信息

        :param X: numpy.ndarray(m, n), input data
        :param y: numpy.ndarray(m, 1), data label, the value is 1 or 0
        :return: str, the classification_report
        """
        y_predict = self.predict(X)
        rep = classification_report(y, y_predict)

        return rep

    def plot_decision_boundary(self, X, y):
        """
        绘制决策边界

        :param X: numpy.ndarray(m, 2), the x of X must be 2-D
        :param y: numpy.ndarray(m, 1), the value is 1 or 0
        :return: fig, ax
        """
        positive_idx, negative_idx = np.where(y == 1)[0], np.where(y == 0)[0]
        positive, negative = X[positive_idx], X[negative_idx]

        fig, ax = plt.subplots(figsize=(10, 6))

        ax.scatter(positive[:, 0], positive[:, 1], s=30, c='g', marker='+', label='Positive')
        ax.scatter(negative[:, 0], negative[:, 1], s=30, c='r', marker='o', label='Negative')

        x_left, x_right = X[:, 0].min(), X[:, 0].max()
        y_left, y_right = X[:, 1].min(), X[:, 1].max()
        grid_density = 1000.0
        step_x = (x_right - x_left) / grid_density
        step_y = (y_right - y_left) / grid_density
        xx, yy = np.meshgrid(np.arange(x_left, x_right, step_x), np.arange(y_left, y_right, step_y))

        X_mat = np.c_[xx.ravel(), yy.ravel()]
        X_with_features = self.poly.fit_transform(X_mat)
        theta = self.result.x

        z = X_with_features @ theta
        z = z.reshape(xx.shape)

        ax.contour(xx, yy, z, 0)

        ax.legend()
        # plt.show()

        return fig, ax


class OneVsAllClassification(LogisticRegression):
    """
    利用逻辑回归进行一对多分类。
    继承了 class LogisticRegression。
    """

    def fit(self, X, Y):
        """
        基于LogisticRegression拟合数据(X, Y)，self.result是包含k个LogisticRegression.result的list。

        :param X: numpy.ndarray(m, n), input data.
        :param Y: numpy.ndarray(m, k), k is the number of classification.
        :return: self.
        """
        res = []
        for k in range(Y.shape[1]):
            super(OneVsAllClassification, self).fit(X, Y[:, k].reshape(Y.shape[0], 1))
            res.append(self.result)
        self.result = res

        return self

    def predict(self, X):
        """
        预测数据

        :param X: numpy.ndarray(m, n), input data.
        :return: numpy.ndarray(m, k), the value is 1 or zero.
        """
        X = self.prepare_data(X)
        Theta = np.asarray([res.x for res in self.result]).T
        prob = self.sigmoid(X @ Theta)

        rows = np.arange(X.shape[0], dtype='int64')
        cols = np.argmax(prob, axis=1)
        Y_predict = np.zeros((X.shape[0], Theta.shape[1]))
        Y_predict[rows, cols] = 1

        return Y_predict

    def report(self, X, Y):
        """
        分类报告，可查看准确率等信息

        :param X: numpy.ndarray(m, n), input data.
        :param Y: numpy.ndarray(m, k), k is the number of classification.
        :return: str, the classification_report
        """
        Y_predict = self.predict(X)
        rep = classification_report(Y, Y_predict)

        return rep
